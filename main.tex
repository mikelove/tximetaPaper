\documentclass[12pt]{article} \usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,graphicx}
\usepackage[margin=1in]{geometry} \usepackage{hyperref}
\usepackage{xcolor} \newcommand{\mike}[1]{\textcolor{blue}{(Mike: #1)}}
\usepackage{xcolor} \newcommand{\charlotte}[1]{\textcolor{red}{(Charlotte: #1)}}
\usepackage{xcolor} \newcommand{\peter}[1]{\textcolor{orange}{(Peter: #1)}}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\widowpenalty10000
\clubpenalty10000

\usepackage[sort&compress,numbers]{natbib}
\bibliographystyle{unsrtnat}

\title{Tximeta: reference sequence checksums for \\ provenance
  identification in RNA-seq} \author{Michael I. Love, Charlotte
  Soneson, Peter F. Hickey, Lisa K. Johnson, \\ Tessa Pierce, Lori
  Shepherd, Martin Morgan, Rob Patro}

\begin{document}

\maketitle

\section*{Introduction}

An RNA-seq data analysis often involves quantification of sequence
read data with respect to a set of known reference transcripts. These
reference transcripts may be downloaded from a database such as
GENCODE, Ensembl, or RefSeq \citep{gencode,ensembl,refseq} in the form
of nucleotide sequences in FASTA format and/or transcript locations in a genome in
GTF/GFF (gene transfer format / general feature format). Alternatively
a novel set of reference transcripts may be derived as part of the data analysis. The
provenance of the reference transcripts, including their source and release
number, can be considered critical metadata with respect to the
processed data, that is, the files with quantification information. Without
information about the reference provenance, computational
reproducibility (as defined by \citet{Patil2016} as re-performing the
analysis with the same data and code and obtaining the same result)
may be difficult or impossible. Manually keeping track of these
critical pieces of metadata throughout a long-term bioinformatic
project is tedious and error prone; still, manual metadata tracking is
a common practice in RNA-seq bioinformatics. For example, a common
approach to tracking the reference transcripts that were used during
quantification would be to keep a \texttt{README} file in the same
directory as the quantification data, with information about the
provenance of the reference transcripts.

In addition to impeding computational reproducibility, missing or
wrong metadata can potentially lead to serious errors in downstream
analysis: if quantification data are shared with genomic coordinates
but without criticial metadata about the genome version, computation of
overlaps with other genomic data with mis-matching genome versions can
lead to faulty inference of overlap enrichment. Additional annotation
tasks, such as conversion of transcript or gene identifiers, or
summarization of transcript-level data to the gene level is made more difficult
when the reference provenance is not known. \citet{Kanduri2017}
documented issues surrounding the lack of provenance metadata for BED,
WIG, and GFF files, and described this problem as a ``major time
thief'' in bioinformatics. Likewise, \citet{Simoneau2019} described
information on genome assembly and annotation as ``essential'' for
describing the computational analysis of RNA-seq data, and contended
that, ``no study using RNA-seq should be published without these
methodological details.''

A number of frameworks have been proposed that would solve the problem
of tracking provenance in a bioinformatic analysis -- provenance in
the narrow sense defined above, encompassing the source and release
information of the reference sequence -- as well as in a larger sense
of tracking the state of all files, including data and metadata and
any software used to process these files, throughout every step of an
analysis. We will first review frameworks for tracking provenance of
reference sequences, and secondly describe more general
frameworks. The CRAM format, developed at the European Bioinformatics
Institute, involves computing differences between biological sequences
and a given reference so that the sequences themselves do not need to
be stored in full within an alignment file \citep{cram}. Because the
specific reference used for compression is critical for data
integrity, CRAM includes checksums of the reference sequences as part
of the file header. A partner utility called refget has been developed
in order to allow for programmatic retrieval of the reference sequence
from a computed checksum, which acts as an identifier of the reference
sequence \citep{refget}. A similar approach is taken by the Global
Alliance for Genomics and Health's (GA4GH) Variation Representation
Specification (VR-Spec) \citep{vr}, which uses a hashed checksum (or
``digest'') to uniquely refer to molecular variation, and by the
seqrepo python package for writing and reading collections of
biological sequences \citep{seqrepo}. The NCBI Assembly database takes
a different approach, by assigning unambiguous identifier strings
(though not computed via a hash function) to sets of sequences
comprising specific releases of a genome assembly
\citep{ncbi-assembly}. Knowing the identifier is therefore sufficient
to knowing the full set of sequences in the assembly.
Another approach to reduce manual metadata tracking associated with a
number of reference sequences is Refgenie. Refgenie is a tool that
helps with management of bundles of files associated with reference
genomes, and facilitates sharing provenance information across
research groups, in that the generation of resources is scripted
\citep{refgenie}. Arkas, ARMOR, pepkit, and basejump are all
frameworks for automating bioinformatic analyses, where reference
provenance is specified in configuration files and correct metadata
can therefore be assembled and attached programmatically to downstream
outputs \citep{arkas,Orjuelag2019,pepkit,basejump}.

In 2015, \citet{Belhajjame2015} introduced the concept of a ``Research
Object'', an aggregation of data and supporting metadata produced
within a specified scientific workflow. Their formulation was
system-neutral, describing the requirements for production of a
Research Object. The requirements touch on topics introduced above,
such as the need to preserve data inputs, software versions, as well
as traces of the provenance of data as it moves through the scientific
workflow. \citet{Belhajjame2015} summarized literature in the field of
computational reproducibility and efforts toward extensive provenance
tracking. The developers of the Common Workflow Language (CWL) \citep{cwl}
have defined a profile, CWLProv, for recording provenance through a
workflow run, and have a number of implementations, including within
cwltool \citep{Khan2018}. The developers of CWLProv emphasized the
importance of tracking versions of input data, such as reference
genomes or variant databases in a scientific workflow, and they
suggested to use and store stable identifiers of all data and software,
as well as the workflow itself. As identifiers play such a crucial
role in assuring reproducibility of workflows, the developers of
CWLProv recommended the use of hashed checksums for identifiers of data
(including any reference sequence), similar to the use of checksums in
the CRAM format and VR-Spec, for identifying the reference or variant
sequences. \citet{Gruning2018} recommended combining systems such as
Galaxy for encapsulating analysis tools with systems for tracking and
capturing parameters and source data provenance to provide full
computational reproducibility.

Here we describe an R/Bioconductor package, tximeta, for identification
of reference transcript provenance in RNA-seq analyses via sequence
checksums. It is situated among other solutions for facilitating
computational reproducibility described above, with some automation of
routine tasks, such as conversion of transcript and gene names, but
short of full automation of analyses as in Arkas and ARMOR. Tximeta
captures the versions of the software packages used in import of
quantification data, but does not provide full provenance tracking
throughout downstream tasks as in the Research Object specification or
in CWLProv. One desirable aspect of tximeta is that -- through the use
of hashed checksums of reference transcripts and lookup operations
similar to those performed by refget -- our implementation can be used
to identify the reference provenance \textit{post hoc} on various
shared or public datasets, regardless of whether the original analyst
kept or shared accurate records of the reference transcripts that were
used. Therefore it can provide some utility for bioinformatic analysts
without requiring full buy-in of a particular workflow execution
framework. Tximeta is similar in implementation to how the CRAM format
uses hashed checksums to identify the particular genome sequence used
during alignment, but instead to identify the transcript sequences used during
RNA-seq sample quantification. We see tximeta as a piece of a larger
effort to create software systems that are ``more amenable to
reproducibility'' \citep{Peng2011}.

\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{diagram.pdf}
  \caption{Flowchart of Salmon quantification followed by tximeta
    data import, resulting in a SummarizedExperiment with reference
    transcript provenance metadata (see Methods).}
  \label{fig:diagram}
\end{figure}

\section*{Methods}

Tximeta has been developed to work with output from Salmon, Sailfish,
or Alevin quantification tools \citep{sailfish,salmon,alevin},
although the implementation could be extended to other quantification
tools that store a hashed checksum within the index and propagate this
checksum to the sample output metadata. Without loss of generality, we
describe the implementation referring to Salmon quantification data
below. A diagram of the following workflow is shown in Figure
\ref{fig:diagram}. 

During the indexing step, Salmon computes the hashed checksum of the
cDNA sequence of the reference transcripts. The set of reference
transcripts provided to Salmon will be referred to in this text as the
\textit{transcriptome}, although we note that the reference is not
necessarily equal to the complete set of possible RNA transcripts in
the sample. Currently, both the SHA-256 and SHA-512 \citep{sha1}
checksums are computed on the reference cDNA sequences alone, with
transcript sequences concatenated together with the empty string (the
SHA-256 checksum is currently taken as the main identifier). Future
implementations of Salmon and tximeta may use alternate hash functions
for compatibility with larger efforts toward stable identifiers for
sequence collections, for example, computing a hashed checksum over a
lexicographically sorted set of checksums for each transcript cDNA
sequence, which would provide order-invariance for the collection
identifier. During quantification of a single sample, Salmon embeds
the transcriptome index checksum in a metadata file associated with
the sample output. Per sample, Salmon outputs a directory with a
specific file structure, including files with quantification
information as well as others with important metadata about the
parameters. The entire directory should be considered the output of
the quantification tool, and not just the text file with the
quantification information.

During import of quantification data into R/Bioconductor
\citep{bioc}, leveraging the existing tximport package
\citep{tximport}, tximeta reads the quantification data, as well as
the transcriptome index checksum, and compares this checksum to a hash
table of pre-computed checksums of a subset of commonly used reference
transcriptomes (human, mouse, and fruit fly reference transcripts from
GENCODE, Ensembl, and RefSeq, see Table \ref{tab1}), as well as to a
custom hash table which will be described below. Tximeta verifies that
the checksum and therefore the reference transcriptome sequence is
identical across all samples being imported. If there is a match of
the checksum among the pre-computed checksums or in the custom hash table,
tximeta will begin to compile additional relevant
metadata. Depending on whether the checksum has been seen by tximeta
before, one of two steps will occur:

\begin{itemize}
\item (First time) - Tximeta attempts to download the appropriate GTF/GFF
  file via FTP and parse it. GENCODE and RefSeq GTF/GFF files are parsed
  by GenomicFeatures \citep{granges}, while Ensembl GTF files are
  parsed by ensembldb \citep{ensembldb}. Tximeta then creates a
  locally cached SQLite database of the parsed GTF/GFF file, as well as a
  GRanges object of the transcript locations \citep{granges}. The
  local cache is managed by the BiocFileCache Bioconductor package
  \citep{biocfilecache}.
\item (Subsequently) - Tximeta loads the locally cached versions of
  metadata (the transcript ranges, or additionally the SQLite database
  on demand for further annotation tasks).
\end{itemize}

\begin{table}[t]
\centering
\begin{tabular}{llll} 
\hline Source & Organism & Releases & Transcript sequence file
\\ \hline\hline GENCODE & \textit{Homo sapiens} & 23 -- 31 &
transcripts.fa \\ GENCODE & \textit{Mus musculus} & M6 -- M22 &
transcripts.fa \\ \hline Ensembl & \textit{Homo sapiens} & 76 -- 97 &
*.cdna.all.fa (nr) \\ Ensembl & \textit{Mus musculus} & 76 -- 97 &
*.cdna.all.fa (nr) \\ Ensembl & \textit{Drosophila melanogaster} & 79
-- 97 & *.cdna.all.fa (nr) \\ \hline Ensembl & \textit{Homo sapiens} &
76 -- 97 & *.cdna.all.fa + *.ncrna.fa \\ Ensembl & \textit{Mus
  musculus} & 76 -- 97 & *.cdna.all.fa + *.ncrna.fa \\ Ensembl &
\textit{Drosophila melanogaster} & 79 -- 97 & *.cdna.all.fa +
*.ncrna.fa \\ \hline RefSeq & \textit{Homo sapiens} & p1 -- p13 &
*\_rna.fa \\ RefSeq & \textit{Mus musculus} & p2 -- p6 & *\_rna.fa
\\ \hline
\end{tabular}
\caption{Pre-computed reference transcripts checksums in tximeta as of
  Fall 2019. The set of pre-computed checksums span the releases from
  these sources for the years 2015 -- 2019. (nr) - not recommended: we
  recommend combination of coding and non-coding transcripts for
  RNA-seq quantification.}
\label{tab1}
\end{table}

After loading the appropriate annotation metadata, tximeta outputs a
SummarizedExperiment object \citep{granges}, a class in the
Bioconductor ecosystem which stores multiple similarly shaped matrices
of data, or ``assays'', including the estimated read counts, effective
transcript lengths, and estimates of abundance (in transcripts per
million, TPM). By convention, rows correspond to genomic features
(e.g. transcripts or genes), while columns correspond to samples. In
addition, the rows of the matrices are linked to transcript ranges,
embedded in an appropriate genome version (e.g. GRCh38) including
chromosome names and lengths. If tximeta did not find a matching
transcriptome in the hash table then a non-ranged SummarizedExperiment
will be returned as the function's output, as the location and context
of the transcript ranges are not known to tximeta. Comparison of
ranges across genome versions, or without properly matching
chromosomes, will produce an error, leveraging default functionality
from the underlying GenomicRanges package \citep{granges}. Metadata
about the samples, if provided by the user, is automatically attached
to the columns of the SummarizedExperiment object. Additional metadata
attached by tximeta includes all of the per-sample metadata saved from
Salmon (e.g. library type, percent reads mapping, etc.), information
about the reference transcriptome and file paths or ftp URLs for the source file(s)
for FASTA and GTF/GFF, and the package versions for tximeta and other
Bioconductor packages used during the parsing of the GTF/GFF. At any
later point in time, annotation tasks can be performed by on-demand
retrieval of the cached databases, for example summarization of
transcript-level information to the gene level, conversion of
transcript or gene identifiers, or addition of exon ranges.

A key aspect of the tximeta workflow described here is that it does
not rely on self-reporting of the reference provenance for
\textit{post hoc} identification of the correct metadata. An exception
to this rule is the case of a \textit{de novo} constructed
transcriptome, or in general, use of a transcriptome that is not yet
contained in tximeta's built-in hash table of reference
transcriptomes. For such cases, we have developed functionality in
tximeta to formally link a given hashed checksum to a publicly
available FASTA file(s) and a GTF/GFF file. The
\texttt{makeLinkedTxome} function can be called, pointing to the
Salmon index as well as the locations of the FASTA and GTF/GFF files, and
this will perform two operations: (1) it will add a row to a custom hash
table, managed by BiocFileCache, and (2) it will produce a JSON file
that can be shared or uploaded, which links the transcriptome checksum
with the source locations. When the JSON file is provided to
\texttt{loadLinkedTxome} on another machine, it will add the relevant
row to tximeta's custom hash table, so tximeta will then recognize and
automatically populate metadata in a similar manner to if the checksum
matched with a transcriptome in tximeta's built-in hash
table. Finally, the cache location for tximeta, managed by
BiocFileCache, can be shared across users on a cluster, for example,
so that parsed databases, range objects, and custom hash tables
created by any one user can be leveraged by all other users.

\section*{Results}

\subsection*{Importing quantification data from known transcriptome}

An example of importing RNA-seq quantification data using tximeta can
be followed in the tximeta or fishpond Bioconductor package
vignettes. Here we demonstrate the case where the Salmon files were
quantified against a transcriptome that is in tximeta's pre-computed
hash table (for a list of supported transcriptomes as of the writing
of this manuscript, see Table \ref{tab1}). Import begins by specifying
a sample table (the ``column data'', as the columns of the
SummarizedExperiment object correspond to samples from the
experiment).

\begin{verbatim}
    coldata <- read.csv("coldata.csv")
\end{verbatim}

For example, in the fishpond Bioconductor package vignette, the
following \texttt{coldata} is read into R in the beginning of the
analysis (here just showing the first two rows and five columns):

\begin{verbatim}
    ##            names sample_id line_id replicate condition_name
    ## 1 SAMEA103885102    diku_A  diku_1         1          naive
    ## 2 SAMEA103885347    diku_B  diku_1         1           IFNg
\end{verbatim}

This table must have a column \texttt{files} which points to paths of
quantification files (\texttt{quant.sf}), and a column \texttt{names} with the sample
identifiers. It is expected that the quantification files are located
within the original directory structure created by Salmon and with all
the associated metadata files. One paradigm used in fishpond vignette
and elsewhere is to construct the \texttt{files} column from the
\texttt{names} column using R's \texttt{file.path} function. The next
step is to provide this table to the \texttt{tximeta} function:

\begin{verbatim}
    se <- tximeta(coldata)
\end{verbatim}

Tximeta will then print the following messages as the files are being
imported (if a match is found, and if this checksum has been seen
before by tximeta):

\begin{verbatim}
    ## importing quantifications reading in files with read_tsv 
    ## found matching transcriptome: 
    ## [ GENCODE - Homo sapiens - release 97 ]
    ## loading existing EnsDb created: 2019-01-01 12:34:56 
    ## loading existing transcript ranges created: 2019-01-01 12:34:56
\end{verbatim}

If the checksum matched one of the custom transcriptome checksums that
was created of loaded by the user, the function would report,
``\texttt{found matching linked transcriptome}''. A demonstration of
such a workflow is given in the following section.

The SummarizedExperiment object, \texttt{se}, that is returned by
tximeta can then be passed to various downstream packages such as
DESeq2, edgeR, limma-voom, or fishpond, with example code in the
tximeta package vignette \citep{deseq2,edger,limma,voom,swish}. The
transcript or gene ranges can be easily manipulated using the
GenonicRanges or plyranges packages in the Bioconductor ecosystem
\citep{granges,Lee2019}. For example, to subset the object to only
those transcripts that overlap a range defined in a variable
\texttt{x}:

\begin{verbatim}
    se_sub <- se[se %over% x,]
\end{verbatim}

Further examples of manipulating the SummarizedExperiment object can
be found in the tximeta vignette, in the fishpond vignette, and in the
plyrangesTximetaCaseStudy package \citep{casestudy}.

\subsection*{Importing data from a \textit{de novo} transcriptome}

It is also possible to use tximeta to import quantification data when
the transcriptome does not belong to those in the set of pre-computed
checksums (Table \ref{tab1}). This case may occur because the
reference transcriptome is from another source or another organism
other than those currently in this pre-computed set, or because the
transcriptome has been modified by the addition of non-reference
transcripts (e.g. cancer fusion transcripts, or pathogen transcripts)
which changes the checksum, or because the entire transcriptome has
been assembled \textit{de novo}. In all of these cases, tximeta
provides a mechanism for local metadata linkage, as well as a formal
mechanism for sharing the link between the quantification data and
publicly available reference transcript files.

The key concept used in the case when the checksum is not part of the
pre-computed set, is that of a link constructed between the
transcriptome used for quantification via its hashed checksum and
publicly accessible metadata locations (i.e. permalinks for the FASTA
and GTF/GFF files). This link is created by the tximeta function
\texttt{makeLinkedTxome} which stores the reference transcriptome's
checksum in a custom hash table managed by BiocFileCache, along with
the permalinks to publicly available FASTA and GTF/GFF files.

We demonstrate this use case, with RNA-seq data of the speckled
killifish (\textit{Fundulus rathbuni}), quantified via Salmon
\citep{salmon} against a \textit{de novo} transcriptome assembled with
Trinity \citep{trinity} and annotated via dammit \citep{dammit}. An
example workflow is provided in the denovo-tximeta repository on
GitHub \citep{denovo}. Here, the FASTA sequence of the
\textit{de novo} assembly as well as a GFF3 annotation file have been
posted to Zenodo, and permalinks are used to point to those
records. After the reference transcripts have been indexed by Salmon,
the following tximeta function can be called within R:

\begin{lstlisting}
makeLinkedTxome(
  indexDir="F_rathbuni.trinity_out", 
  source="dammit",
  organism="Fundulus rathbuni", 
  release="0", 
  genome="none",
  fasta="https://zenodo.org/record/1486276/files/F_rathbuni.trinity_out.fasta",
  gtf="https://zenodo.org/record/2226742/files/F_rathbuni.trinity_out.Trinity.fasta.dammit.gff3",
  jsonFile="F_rathbuni.json"
)
\end{lstlisting}

The function does not return an R object, but has the side effect of
storing an entry in the custom hash table managed by BiocFileCache,
and producing a JSON file which can be shared with other analysts. The
JSON file can be loaded with \texttt{loadLinkedTxome}, and it will
likewise store an entry in the custom hash table of the machine where
it is loaded. In either case, when the quantification data is later
imported using tximeta, the checksum will be recognized and the
relevant metadata attached to the SummarizedExperiment object output:

\begin{verbatim}
   coldata <- data.frame(files, names) 
   se <- tximeta(coldata)

   ## importing quantifications reading in files with read_tsv
   ## found matching linked transcriptome: 
   ## [ dammit - Fundulus rathbuni - release 0 ] 
   ## loading existing TxDb created: 2018-12-13 18:26:20 
   ## generating transcript ranges
\end{verbatim}

After running \texttt{tximeta}, the SummarizedExperiment object
\texttt{se} will have attached to its rows the ranges described by the
GTF/GFF object, including any metadata about those transcripts. In the
case of the killifish RNA-seq experiment, the transcript ranges have
length, strand, and an informative column \texttt{gene\_id}.

\section*{Discussion}

We outline an implementation for importing RNA-seq quantification
data that involves (1) the quantification tool (here, Salmon)
computing a hashed checksum of the reference transcript sequences,
which are embedded in the index and in the per-sample output metadata,
followed by (2) downstream comparison of checksums with a hash table
(here, by tximeta), automated downloading and parsing of the
appropriate metadata, and attachment to a rich object that bundles
data and reference sequence metadata. The software is implemented
within the R/Bioconductor environment for data analysis, and leverages
a number of existing Bioconductor packages for parsing annotation
files, metadata storage, and genomic range manipulation
\citep{bioc,ensembldb,biocfilecache,granges}.

Currently, the pre-computed hashed checksums are focused on human,
mouse, and fruit fly reference transcripts, from the popular reference
transcriptome sources GENCODE, Ensembl, and RefSeq. Additional
transcriptome releases from these sources are programmatically
downloaded, the hashed checksum computed, and the checksum added to
the tximeta package on Bioconductor's 6 month release cycle. We are
hopeful that future integration of tximeta with reference sequence
retrieval efforts from the GA4GH consoritium will allow for a wide
expansion of the number of supported organisms. Potentially all of the
releases of reference transcriptomes from Ensembl and/or RefSeq may be
supported by a future reference sequence retrieval API (GENCODE
releases since 2015 are already fully supported by
tximeta). Furthermore, we provide a mechanism for formally linking
those reference transcripts not in any pre-computed hash table
(e.g. \textit{de novo} transcriptomes) with publicly accessible
metadata. Finally, we plan to develop tximeta to support provenance
identification at the level of alleles, by combining our current
reference transcript identification with transcript variant
identification as described in GA4GH's Variant Representation
Specification \citep{vr}.

All bioinformatic software packages have a finite lifespan, including
the package described here. We join with others in
recommending the underlying paradigm of embedding reference sequence
checksums in sample file metadata, followed by downstream database
lookup of checksums, and identification of reference sequence
metadata. This paradigm should be adopted by other bioinformatic
software that outputs any data that refers to a reference
sequence. This workflow has the advantage of not requiring additional
effort or actions on the part of the upstream bioinformatic
analyst. Otherwise, we risk exposing downstream bioinformatic analysts
to the ``major time thief'' of \textit{post hoc} guesswork involved in
identifying the reference sequence provenance of genomic datasets
shared publicly but without this critical information
\citep{Kanduri2017}.

\section*{Acknowledgments}

The authors thank the following individuals for useful discussions 
in the development of tximeta: Vince Carey, Paul Flicek, Joel Parker,
Oliver Hoffmann, Stephen Turner, Shannan Ho Sui, Thomas Keane, Andy
Yates, Reece Hart, Matthew Laird, Terence Murphy, Nathan Sheffield.

\nocite{*}

\bibliography{refs}

\end{document}
